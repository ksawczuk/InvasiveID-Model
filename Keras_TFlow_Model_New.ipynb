{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some standard packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "from random import shuffle\n",
    "\n",
    "# Importing tf tools\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, MaxPooling2D, Activation, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ksawczuk/python-repo/InvasiveId'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the local directory structure.\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['giant hogweed', 'blueweed', 'gorse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Although I've added a decent number of images to my project, I still consider the dataset to be fairly small for an image classification project.\n",
    "<br>\n",
    "Therefore, I've decided to employ the ImageDataGenerator which allows for batches of images to be transformed and augmented in line and fed to the model one at a time.\n",
    "This will assist with memory management and should allow the maximum amount of info regarding each image to end up in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Keras ImageDataGenerator for memory efficiency and preprocessing ease\n",
    "# This process replaces the method of obtaining our data via DataLoader.ipynb\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4862 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                                    'data/final_BC_images/train',\n",
    "                                                    target_size=(32, 32),\n",
    "                                                    color_mode='rgb',\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True,\n",
    "                                                    subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1215 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                                                        'data/final_BC_images/train/',\n",
    "                                                        target_size=(32, 32,),\n",
    "                                                        color_mode='rgb',\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        shuffle=False,\n",
    "                                                        subset='validation'\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1632 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "                                                  'data/final_BC_images/test/',\n",
    "                                                  target_size=(32, 32),\n",
    "                                                  color_mode='rgb',\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step size = 151 \n",
      "Validation step size = 37 \n",
      "Test step size = 51\n"
     ]
    }
   ],
   "source": [
    "# Saving the number of stepsizes for the training, validation and test sets \n",
    "train_stepsize = train_generator.samples//train_generator.batch_size \n",
    "\n",
    "valid_stepsize = validation_generator.samples//validation_generator.batch_size \n",
    "\n",
    "test_stepsize = test_generator.samples//test_generator.batch_size \n",
    "\n",
    "# Sanity check \n",
    "print(f'Training step size = {train_stepsize} \\nValidation step size = {valid_stepsize} \\nTest step size = {test_stepsize}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1.  Import the pretrained VGG16 network, do not include the top layers\n",
    "pretrained_VGG = VGG16(weights='imagenet', include_top=False, pooling='max', input_shape=(64, 64, 3))\n",
    "\n",
    "# 2.  Setting all layers to not trainable so weights wont be tweaked\n",
    "for layer in pretrained_VGG.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "# Display VGG16 architecture\n",
    "pretrained_VGG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the VGG16 NN model \n",
    "weeds_VGG = Sequential()\n",
    "\n",
    "# Add the pretrained layers \n",
    "weeds_VGG.add(pretrained_VGG) \n",
    "\n",
    "# Add fully-connected dense layers -- plus a dropout layer to help prevent overfitting\n",
    "weeds_VGG.add(Dense(256, activation='relu'))\n",
    "weeds_VGG.add(Dropout(0.5))\n",
    "weeds_VGG.add(Dense(512, activation='relu'))\n",
    "\n",
    "# Adding our activation \n",
    "weeds_VGG.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initiate early stop based on validation accuracy\n",
    "ES = EarlyStopping(monitor='val_acc', patience=5, mode='auto', min_delta=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Istantiating Adam optimizer with a learning rate of 0.0001 and saving to variable 'optim'\n",
    "optim = Adam(lr=0.0001)\n",
    "\n",
    "# Compiling the CNN model \n",
    "weeds_VGG.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Summary \n",
    "weeds_VGG.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### VGG16 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Fitting the model to the training data\n",
    "history_VGG_2nd = weeds_VGG.fit_generator(generator=train_generator,\n",
    "                                steps_per_epoch=train_stepsize,\n",
    "                                epochs=50,\n",
    "                                validation_data=validation_generator,\n",
    "                                validation_steps=valid_stepsize,\n",
    "                                callbacks=[ES])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### VGG16 Results (lr = 0.01)\n",
    "* Learning Rate set to 0.01\n",
    "* named 1st for 1st iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Getting bestmodel's predictions (as probabilities) on the test set \n",
    "test_probas_VGG_1st = weeds_VGG.predict_generator(test_generator, steps=test_stepsize)\n",
    "\n",
    "# Setting the model's class prediction as the class that received the highest probability for each image\n",
    "test_predictions_VGG_1st = test_probas_VGG_1st.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Getting the true class labels for the test set\n",
    "test_true_VGG = test_generator.classes\n",
    "\n",
    "# Sanity check \n",
    "test_true_VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Look at what our model predicted\n",
    "test_predictions_VGG_1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Displaying the classification report for the test set\n",
    "print('Classification Report\\n \\n', classification_report(test_true_VGG, test_predictions_VGG_1st, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get a confusion matrix \n",
    "test_matrix_VGG_1st = pd.DataFrame(confusion_matrix(test_true_VGG, test_predictions_VGG_1st), \n",
    "                           columns=['Predicted ' + cat_name for cat_name in categories], \n",
    "                           index=['True ' + cat_name for cat_name in categories])\n",
    "\n",
    "# Plotting as a heatmap \n",
    "plt.figure()\n",
    "sns.heatmap(test_matrix_VGG_1st, cmap='Blues', annot=True, fmt='g')\n",
    "plt.title('VGG 16: Normalized Confusion Matrix: Test Data(lr=0.01)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Loss and Accuracy per Epoch \n",
    "learning rate set to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0,18,1), history_VGG_1st.history['loss'], label = 'train loss lr:0.01', c = 'b')\n",
    "plt.plot(range(0,18,1), history_VGG_1st.history['val_loss'], label = 'val loss lr:0.01', c = 'r')\n",
    "plt.plot(range(0,20,1), history_VGG_2nd.history['loss'], label = 'val loss lr:0.0001', c = 'g')\n",
    "plt.plot(range(0,20,1), history_VGG_2nd.history['val_loss'], label = 'val loss lr:0.0001', c = 'y')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0,18,1), history_VGG_1st.history['acc'], label = 'train acc lr:0.01', c = 'b')\n",
    "plt.plot(range(0,18,1), history_VGG_1st.history['val_acc'], label = 'val acc lr:0.01', c = 'r')\n",
    "plt.plot(range(0,20,1), history_VGG_2nd.history['acc'], label = 'train acc lr:0.0001', c = 'g')\n",
    "plt.plot(range(0,20,1), history_VGG_2nd.history['val_acc'], label = 'val acc lr:0.0001', c = 'y')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy (lr=0.01)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### VGG16 Results (lr = 0.0001)\n",
    "Learning Rate set to 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Getting bestmodel's predictions (as probabilities) on the test set \n",
    "test_probas_VGG_2nd = weeds_VGG.predict_generator(test_generator, steps=test_stepsize)\n",
    "\n",
    "# Setting the model's class prediction as the class that received the highest probability for each image\n",
    "test_predictions_VGG_2nd = test_probas_VGG_2nd.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Look at what our model predicted\n",
    "test_predictions_VGG_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Displaying the classification report for the test set\n",
    "print('Classification Report\\n \\n', classification_report(test_true_VGG, test_predictions_VGG_2nd, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get a confusion matrix \n",
    "test_matrix_VGG_2nd = pd.DataFrame(confusion_matrix(test_true_VGG, test_predictions_VGG_2nd), \n",
    "                           columns=['Predicted ' + cat_name for cat_name in categories], \n",
    "                           index=['True ' + cat_name for cat_name in categories])\n",
    "\n",
    "# Plotting as a heatmap \n",
    "plt.figure()\n",
    "sns.heatmap(test_matrix_VGG_2nd, cmap='Blues', annot=True, fmt='g')\n",
    "plt.title('VGG 16: Normalized Confusion Matrix: Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Loss and Accuracy per Epoch \n",
    "learning rate set to 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0,20,1), history_VGG_2nd.history['loss'], label = 'train loss', c = 'b')\n",
    "plt.plot(range(0,20,1), history_VGG_2nd.history['val_loss'], label = 'val loss', c = 'r')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss (lr=0.0001)')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0,20,1), history_VGG_2nd.history['acc'], label = 'train acc', c = 'b')\n",
    "plt.plot(range(0,20,1), history_VGG_2nd.history['val_acc'], label = 'val acc', c = 'r')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy (lr=0.0001)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1.  Import the pretrained VGG16 network, do not include the top layers\n",
    "pretrained_VGG = VGG16(weights='imagenet', include_top=False, pooling='max', input_shape=(32, 32, 3))\n",
    "\n",
    "# 2.  Setting all layers to not trainable so weights wont be tweaked\n",
    "for layer in pretrained_VGG.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "# Display VGG16 architecture\n",
    "pretrained_VGG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the VGG16 NN model \n",
    "VGG = Sequential()\n",
    "\n",
    "# Add the pretrained layers \n",
    "VGG.add(pretrained_VGG) \n",
    "\n",
    "# Add fully-connected dense layers -- plus a dropout layer to help prevent overfitting\n",
    "# 1st perform batch normalizaiton:\n",
    "\n",
    "VGG.add(Dense(256, activation='relu'))\n",
    "VGG.add(BatchNormalization())\n",
    "VGG.add(Dense(512, activation='relu'))\n",
    "VGG.add(Dropout(0.5))\n",
    "VGG.add(Dense(256, activation='relu'))\n",
    "\n",
    "# Adding our activation \n",
    "VGG.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate early stop based on validation accuracy\n",
    "ES = EarlyStopping(monitor='val_loss', patience=5, mode='auto', min_delta=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 512)               14714688  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 15,110,723\n",
      "Trainable params: 395,523\n",
      "Non-trainable params: 14,715,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Istantiating Adam optimizer with a learning rate of 0.0001 and saving to variable 'optim'\n",
    "optim = Adam(lr=0.0001)\n",
    "\n",
    "# Compiling the CNN model \n",
    "VGG.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Summary \n",
    "VGG.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-8bdb2c944341>:2: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "151/151 [==============================] - 101s 668ms/step - loss: 0.7476 - acc: 0.6889 - val_loss: 0.5142 - val_acc: 0.8015\n",
      "Epoch 2/50\n",
      "151/151 [==============================] - 94s 625ms/step - loss: 0.5445 - acc: 0.7857 - val_loss: 0.4656 - val_acc: 0.8176\n",
      "Epoch 3/50\n",
      "151/151 [==============================] - 94s 624ms/step - loss: 0.4895 - acc: 0.8097 - val_loss: 0.4420 - val_acc: 0.8353\n",
      "Epoch 4/50\n",
      "151/151 [==============================] - 97s 643ms/step - loss: 0.4534 - acc: 0.8246 - val_loss: 0.4396 - val_acc: 0.8328\n",
      "Epoch 5/50\n",
      "151/151 [==============================] - 101s 667ms/step - loss: 0.4534 - acc: 0.8263 - val_loss: 0.4151 - val_acc: 0.8480\n",
      "Epoch 6/50\n",
      "151/151 [==============================] - 100s 662ms/step - loss: 0.4373 - acc: 0.8275 - val_loss: 0.4325 - val_acc: 0.8438\n",
      "Epoch 7/50\n",
      "151/151 [==============================] - 100s 663ms/step - loss: 0.4340 - acc: 0.8261 - val_loss: 0.4030 - val_acc: 0.8463\n",
      "Epoch 8/50\n",
      "151/151 [==============================] - 101s 669ms/step - loss: 0.4172 - acc: 0.8383 - val_loss: 0.3844 - val_acc: 0.8539\n",
      "Epoch 9/50\n",
      "151/151 [==============================] - 101s 668ms/step - loss: 0.4137 - acc: 0.8420 - val_loss: 0.3902 - val_acc: 0.8556\n",
      "Epoch 10/50\n",
      "151/151 [==============================] - 101s 666ms/step - loss: 0.4244 - acc: 0.8377 - val_loss: 0.3878 - val_acc: 0.8454\n",
      "Epoch 11/50\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 0.4147 - acc: 0.8342 - val_loss: 0.3850 - val_acc: 0.8564\n",
      "Epoch 12/50\n",
      "151/151 [==============================] - 101s 670ms/step - loss: 0.3918 - acc: 0.8443 - val_loss: 0.3707 - val_acc: 0.8547\n",
      "Epoch 13/50\n",
      "151/151 [==============================] - 101s 669ms/step - loss: 0.3962 - acc: 0.8466 - val_loss: 0.3798 - val_acc: 0.8590\n",
      "Epoch 14/50\n",
      "151/151 [==============================] - 103s 681ms/step - loss: 0.3817 - acc: 0.8518 - val_loss: 0.3550 - val_acc: 0.8606\n",
      "Epoch 15/50\n",
      "151/151 [==============================] - ETA: 0s - loss: 0.3926 - acc: 0.8468"
     ]
    }
   ],
   "source": [
    "# Fitting the model to the training data\n",
    "history_VGG = VGG.fit_generator(generator=train_generator,\n",
    "                        steps_per_epoch=train_stepsize,\n",
    "                        epochs=50,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=valid_stepsize,\n",
    "                        callbacks=[ES])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning\n",
    "Unfreeze all the layers of the pretrained model, recompile without rebuilding to ensure normalization values aren't lost.<br>\n",
    "Retrain on the data to provide better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_VGG.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Istantiating Adam again optimizer with a learning rate of 0.0001 and saving to variable 'optim'\n",
    "optim = Adam(lr=0.00001) \n",
    "\n",
    "# Compiling the CNN model \n",
    "VGG.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Summary \n",
    "VGG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model to the training data\n",
    "history_VGG_fine_tune = VGG.fit_generator(generator=train_generator,\n",
    "                        steps_per_epoch=train_stepsize,\n",
    "                        epochs=10,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=valid_stepsize,\n",
    "                        callbacks=[ES])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 Results (lr = 0.0001)\n",
    "Learning Rate set to 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Getting bestmodel's predictions (as probabilities) on the test set \n",
    "test_probas_VGG_2nd = weeds_VGG.predict_generator(test_generator, steps=test_stepsize)\n",
    "\n",
    "# Setting the model's class prediction as the class that received the highest probability for each image\n",
    "test_predictions_VGG_2nd = test_probas_VGG_2nd.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at what our model predicted\n",
    "test_predictions_VGG_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the classification report for the test set\n",
    "print('Classification Report\\n \\n', classification_report(test_true_VGG, test_predictions_VGG_2nd, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a confusion matrix \n",
    "test_matrix_VGG_2nd = pd.DataFrame(confusion_matrix(test_true_VGG, test_predictions_VGG_2nd), \n",
    "                           columns=['Predicted ' + cat_name for cat_name in categories], \n",
    "                           index=['True ' + cat_name for cat_name in categories])\n",
    "\n",
    "# Plotting as a heatmap \n",
    "plt.figure()\n",
    "sns.heatmap(test_matrix_VGG_2nd, cmap='Blues', annot=True, fmt='g')\n",
    "plt.title('VGG 16: Normalized Confusion Matrix: Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Accuracy per Epoch \n",
    "learning rate set to 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0,20,1), history_VGG_2nd.history['loss'], label = 'train loss', c = 'b')\n",
    "plt.plot(range(0,20,1), history_VGG_2nd.history['val_loss'], label = 'val loss', c = 'r')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss (lr=0.0001)')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0,20,1), history_VGG_2nd.history['acc'], label = 'train acc', c = 'b')\n",
    "plt.plot(range(0,20,1), history_VGG_2nd.history['val_acc'], label = 'val acc', c = 'r')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy (lr=0.0001)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
