{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some standard packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# modelling packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['hogweed', 'blueweed', 'gorse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Keras ImageDataGenerator for memory efficiency and preprocessing ease\n",
    "# This process replaces the method of obtaining our data via DataLoader.ipynb\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rescale=1./255,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4862 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                                    'data/final_BC_images/train',\n",
    "                                                    target_size=(32, 32),\n",
    "                                                    color_mode='rgb',\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True,\n",
    "                                                    subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1215 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                                                        'data/final_BC_images/train',\n",
    "                                                        target_size=(32, 32),\n",
    "                                                        color_mode='rgb',\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        shuffle=False,\n",
    "                                                        subset='validation'\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1632 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "                                                  'data/final_BC_images/test',\n",
    "                                                  target_size=(32, 32),\n",
    "                                                  color_mode='rgb',\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step size = 151 \n",
      "Validation step size = 37 \n",
      "Test step size = 51\n"
     ]
    }
   ],
   "source": [
    "# Saving the number of stepsizes for the training, validation and test sets \n",
    "train_stepsize = train_generator.samples//train_generator.batch_size \n",
    "\n",
    "valid_stepsize = validation_generator.samples//validation_generator.batch_size \n",
    "\n",
    "test_stepsize = test_generator.samples//test_generator.batch_size \n",
    "\n",
    "# Sanity check \n",
    "print(f'Training step size = {train_stepsize} \\nValidation step size = {valid_stepsize} \\nTest step size = {test_stepsize}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "\n",
    "# 3 convolutional layers\n",
    "model.add(Conv2D(32, (3,3), input_shape = (32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 3 hidden layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# # The output layer with 9 neurons for 9 classes\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate early stop based on validation accuracy\n",
    "ES = EarlyStopping(monitor='val_acc', patience=5, mode='auto', min_delta=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 128)       36992     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 512)         590336    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 1,251,971\n",
      "Trainable params: 1,251,907\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Istantiating Adam optimizer with a learning rate of 0.0001 and saving to variable 'optim'\n",
    "optim = Adam(lr=0.001)\n",
    "\n",
    "# Compiling the CNN model \n",
    "model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-99fec99dcb13>:2: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "151/151 [==============================] - 61s 404ms/step - loss: 0.6806 - acc: 0.7076 - val_loss: 0.9785 - val_acc: 0.4890\n",
      "Epoch 2/50\n",
      "151/151 [==============================] - 58s 382ms/step - loss: 0.4913 - acc: 0.8124 - val_loss: 0.8301 - val_acc: 0.5760\n",
      "Epoch 3/50\n",
      "151/151 [==============================] - 59s 392ms/step - loss: 0.4021 - acc: 0.8578 - val_loss: 0.5431 - val_acc: 0.7821\n",
      "Epoch 4/50\n",
      "151/151 [==============================] - 58s 386ms/step - loss: 0.3815 - acc: 0.8540 - val_loss: 0.6574 - val_acc: 0.6917\n",
      "Epoch 5/50\n",
      "151/151 [==============================] - 59s 388ms/step - loss: 0.3604 - acc: 0.8654 - val_loss: 0.4090 - val_acc: 0.8530\n",
      "Epoch 6/50\n",
      "151/151 [==============================] - 57s 380ms/step - loss: 0.3465 - acc: 0.8752 - val_loss: 0.8646 - val_acc: 0.7171\n",
      "Epoch 7/50\n",
      "151/151 [==============================] - 59s 388ms/step - loss: 0.3411 - acc: 0.8720 - val_loss: 0.4019 - val_acc: 0.8590\n",
      "Epoch 8/50\n",
      "151/151 [==============================] - 61s 402ms/step - loss: 0.3467 - acc: 0.8752 - val_loss: 0.4490 - val_acc: 0.8226\n",
      "Epoch 9/50\n",
      "151/151 [==============================] - 61s 406ms/step - loss: 0.3558 - acc: 0.8714 - val_loss: 0.5027 - val_acc: 0.8074\n",
      "Epoch 10/50\n",
      "151/151 [==============================] - 60s 399ms/step - loss: 0.3622 - acc: 0.8634 - val_loss: 0.9555 - val_acc: 0.6427\n",
      "Epoch 11/50\n",
      "151/151 [==============================] - 61s 405ms/step - loss: 0.3692 - acc: 0.8619 - val_loss: 0.6556 - val_acc: 0.7154\n",
      "Epoch 12/50\n",
      "151/151 [==============================] - 60s 399ms/step - loss: 0.3759 - acc: 0.8476 - val_loss: 0.9018 - val_acc: 0.6647\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model to the training data\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                                steps_per_epoch=train_stepsize,\n",
    "                                epochs=50,\n",
    "                                validation_data=validation_generator,\n",
    "                                validation_steps=valid_stepsize,\n",
    "                                callbacks=[ES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look promisinng for some epochs, but the validation loss and accuarcy are hopping around way too much. The model seems to be unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting bestmodel's predictions (as probabilities) on the test set \n",
    "test_probas = model.predict_generator(test_generator, steps=test_stepsize)\n",
    "\n",
    "# Setting the model's class prediction as the class that received the highest probability for each image\n",
    "test_predictions = test_probas.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the true class labels for the test set\n",
    "test_true = test_generator.classes\n",
    "\n",
    "# Sanity check \n",
    "len(test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the classification report for the test set\n",
    "print('Classification Report\\n \\n', classification_report(test_true, test_predictions, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a confusion matrix \n",
    "test_matrix = pd.DataFrame(confusion_matrix(test_true, test_predictions), \n",
    "                           columns=['Predicted ' + cat_name for cat_name in categories], \n",
    "                           index=['True ' + cat_name for cat_name in categories])\n",
    "\n",
    "# Plotting as a heatmap \n",
    "plt.figure()\n",
    "sns.heatmap(test_matrix, cmap='Blues', annot=True, fmt='g')\n",
    "plt.title('Normalized Confusion Matrix: Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# history = model.fit_generator(generator=train_generator,\n",
    "#                     steps_per_epoch=(11209) // batch_size,\n",
    "#                     epochs=50, \n",
    "#                     validation_data=validation_generator,\n",
    "#                     validation_steps=(2798) // batch_size,\n",
    "#                     callbacks=[\n",
    "#                         EarlyStopping(patience=3, restore_best_weights=True),\n",
    "#                         ReduceLROnPlateau(patience=2)],\n",
    "#                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
