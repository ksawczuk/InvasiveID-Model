{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# For http interfaces and APIs\n",
    "import requests\n",
    "from flickrapi import FlickrAPI\n",
    "\n",
    "# System I/O controls and clock packages\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FilckrAPI Pic Collector Implementation\n",
    "1. Takes a dictionary of search terms and downloads images\n",
    "2. Creates a directory of each search term where images are stored (data/{search-term-directory}/{image-file}). \n",
    "3. Creates a dictionary of information regarding images gathered (not as important as it could be*).\n",
    "<br> *Because I am using manual selection to remove images that don't apply, it will be required to build \n",
    "<br> the labels.csv file after the manual selection has occurred. As a result, I will build the labels.csv \n",
    "<br> file after the image collection process has been completed in the DataLoader.ipynb file.\n",
    "\n",
    "Due to the errors that crop up from time to time while running this task, (most often connection reset by peer),\n",
    "<br> I have found that this is more of a manually managed process than fully automated.\n",
    "<br>I kick off a search with the full dictionary of target plants and then end up removing species from the dictionary\n",
    "<br>once I have enough images collected prior to restarting the process to collect the remaining species images.\n",
    "<br>\n",
    "<br>\n",
    "Future plans to create a script that will take search inputs, number of images and directory name for easier handling.\n",
    "\n",
    "### Important Note: FlickrAPI Throughput\n",
    "FlickrAPI need to stay below 3600 queries per hour. In order to be conservative, I've put in a 1s delay between image downloads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load key and secret\n",
    "f_key = open('Key.txt', encoding='utf-8')\n",
    "f_secret = open('Secret.txt', encoding='utf-8')\n",
    "KEY = f_key.read()\n",
    "SECRET = f_secret.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I only want images of format: url_q so have removed the other image size formats.\n",
    "\n",
    "SIZES = ['url_q']  # in order of preference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the API comms with key and secret, search for public photos based on dictionary search terms.\n",
    "\n",
    "def get_photos(image_tag):\n",
    "    extras = SIZES\n",
    "    flickr = FlickrAPI(KEY, SECRET)\n",
    "    photos = flickr.walk(text=image_tag,  # it will search by image title and image tags\n",
    "                            extras=extras,  # get the urls for each size we want\n",
    "                            privacy_filter=1,  # search only for public photos\n",
    "                            per_page=50,\n",
    "                            sort='relevance')  # we want what we are looking for to appear first\n",
    "    return photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method gets a single url and handles the case where multiple image sizes are searched for.\n",
    "# (in the case an image doesn't exist in one size, try another from the list). \n",
    "def get_url(photo):\n",
    "    for i in range(len(SIZES)):  # makes sure the loop is done in the order we want\n",
    "        url = photo.get(SIZES[i])\n",
    "        if url:  # if url is None try with the next size\n",
    "            return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will create a list of valid urls per plant with the max number being the value of images_per_plant\n",
    "# inputs image_tag which is the current search term, max which is the images_per_plant value.\n",
    "# returns a list of urls.\n",
    "\n",
    "def get_urls(image_tag, max):\n",
    "    photos = get_photos(image_tag)\n",
    "    counter=0\n",
    "    urls=[]\n",
    "\n",
    "    for photo in photos:\n",
    "        if counter < max:\n",
    "            url = get_url(photo)  # get preffered size url\n",
    "            if url:\n",
    "                urls.append(url)\n",
    "                counter += 1\n",
    "            # if no url for the desired sizes then try with the next photo\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will create directories (if they don't already exist) based on the search terms included in the input dictionary to the download method.\n",
    "\n",
    "def create_folder(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method downloads the images and stores them in the correct directories.\n",
    "# It will ignore the file if it already exists.\n",
    "# It will return a dictionary of filename, class and species though this isn't used currently.\n",
    "\n",
    "def download_images(urls, path, plant):\n",
    "    create_folder(path)  # makes sure path exists\n",
    "    \n",
    "    # Create dictionary of values to keep track of file names / paths, species, class\n",
    "    images_dict = {'filename':[], 'class':[], 'species':[]}\n",
    "\n",
    "    for url in urls:\n",
    "        image_name = url.split(\"/\")[-1]\n",
    "        image_path = os.path.join(path, image_name)\n",
    "        if not os.path.isfile(image_path):  # ignore if already downloaded\n",
    "            response=requests.get(url,stream=True)\n",
    "\n",
    "            with open(image_path,'wb') as outfile:\n",
    "                outfile.write(response.content)\n",
    "            images_dict['filename'].append(image_name)\n",
    "            images_dict['class'].append(plants[plant]) \n",
    "            images_dict['species'].append(plant)\n",
    "        time.sleep(1)\n",
    "    return images_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method kicks it all off. Sets number of images to gather, input is the dictionary of search terms.\n",
    "# Returns dictionary of results\n",
    "images_per_plant = 3000\n",
    "\n",
    "def download(data_dict):\n",
    "    for key in data_dict:\n",
    "\n",
    "        print('Getting urls for', key)\n",
    "        urls = get_urls(key, images_per_plant)\n",
    "        print('Downloading images for', key)\n",
    "        path = os.path.join('data', key)\n",
    "        images_dict = download_images(urls, path, key)\n",
    "    return images_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare inputs for search terms, labels for classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of search terms, scientific name, common name(s) and class.\n",
    "# Heracleum mantegazzianum, giant hogweed class 0\n",
    "# Echium vulgare, blueweed class 1\n",
    "# Ulex europaeus, gorse class 2\n",
    "\n",
    "plants = {'Heracleum mantegazzianum': 0, 'giant hogweed': 0}\n",
    "\n",
    "#  'Echium vulgare':1,'blueweed': 1, 'Ulex europaeus':2, 'gorse': 2\n",
    "# Need a list of class 3 plants, will start with these though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting urls for Heracleum mantegazzianum\n",
      "Downloading images for Heracleum mantegazzianum\n",
      "Getting urls for giant hogweed\n",
      "Downloading images for giant hogweed\n"
     ]
    }
   ],
   "source": [
    "# Calls download method which kicks off the image search and assigns the resulting dictionary to images_dict (currently not used for anything).\n",
    "images_dict = download(plants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Class Images\n",
    "The list of plants below are the species of plants that will make up my 4th class (clsas_3), the negative class.\n",
    "Due to issues with implementing this, I am no longer using this list of species in my project but may add them again later.\n",
    "\n",
    "For now, I will use the images I have collected once I have implemented a probabilities method of determining if a photo is one of the 3 'positive' classes or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Scientific Name</th>\n",
    "        <th>Common Name</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>allium cernuum</td>\n",
    "        <td>nodding onion</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>amsinckia menziesii var. intermedia</td>\n",
    "        <td>common fiddleneck</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>lysimachia thyrsiflora</td>\n",
    "        <td>tufted loosestrife</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>mahonia aquifolium</td>\n",
    "        <td>oregon-grape</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>maianthemum racemosum ssp. amplexicaule</td>\n",
    "        <td>false solomon's-seal</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>monotropa uniflora</td>\n",
    "        <td>indian-pipe</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>oplopanax horridus</td>\n",
    "        <td>devils club plant</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pedicularis contorta var. contorta</td>\n",
    "        <td>coil-beaked lousewort</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>phyllodoce empetriformis</td>\n",
    "        <td>pink mountain-heather</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>potentilla drummondii</td>\n",
    "        <td>drummond's cinquefoil</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>ranunculus acris</td>\n",
    "        <td>meadow buttercup</td>\n",
    "    </tr>\n",
    "</table>   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The null class will be made up of 300 images each of 22 plant species native to BC.\n",
    "\n",
    "class_dict = {'Heracleum mantegazzianum': 0, 'giant hogweed': 0, \n",
    "              'Echium vulgare':1, 'blueweed': 1, 'Ulex europaeus':2, \n",
    "              'gorse': 2, 'allium cernuum':3, 'nodding onion':3, \n",
    "              'amsinckia menziesii var. intermedia':3,\n",
    "              'common fiddleneck':3, 'lysimachia thyrsiflora':3, \n",
    "              'tufted loosestrife':3, 'mahonia aquifolium':3,\n",
    "              'oregon-grape':3, 'maianthemum racemosum ssp. amplexicaule':3,\n",
    "              'false solomon\\'s-seal':3, 'monotropa uniflora':3, \n",
    "              'indian-pipe':3, 'oplopanax horridus':3, \n",
    "              'devils club plant':3, 'pedicularis contorta var. contorta':3,\n",
    "              'coil-beaked lousewort':3, 'phyllodoce empetriformis':3, \n",
    "              'pink mountain-heather':3, 'potentilla drummondii':3,\n",
    "              'drummond\\'s cinquefoil':3, 'ranunculus acris':3, \n",
    "              'meadow buttercup':3} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Data\n",
    "\n",
    "Because of the Internal Server 500 errors when getting the URLs for the positive classes images, I will need to create my label dataframe and save to csv from the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = []\n",
    "image_dict = {}\n",
    "for i, dir in enumerate(dir_names):\n",
    "    image_names.append(os.listdir(f'data/BC-images-clean/{dir_names[i]}'))\n",
    "    image_dict.update({dir_names[i]:image_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file to save to\n",
    "image_dict_pkl = open('data/images-pickle_file2', 'wb')\n",
    " \n",
    "# write the images dictionary to the file\n",
    "pickle.dump(images_dict, images_dict_pkl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
